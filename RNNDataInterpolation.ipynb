{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNNDataInterpolation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wSRJYabvuZL",
        "colab_type": "text"
      },
      "source": [
        "Setup and import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41-6q8Itvnxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from tensorflow.keras import datasets, layers, models, utils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhHkyJ9lvyTA",
        "colab_type": "text"
      },
      "source": [
        "Import data sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJzkUlHHv05w",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "2e48b4c0-b19e-414b-c901-8d508daa4c38"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7756484d-fe13-4c67-95ce-4b45b4c62197\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7756484d-fe13-4c67-95ce-4b45b4c62197\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Train.csv to Train.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-21dc3c638f66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     result = _output.eval_js(\n\u001b[1;32m     71\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n\u001b[0;32m---> 72\u001b[0;31m             output_id=output_id))\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'append'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0;31m# JS side uses a generator of promises to process all of the files- some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qO34Tt-C-VI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "13808e8c-6e7a-4237-c3e2-eb8096ef32aa"
      },
      "source": [
        "df = pd.read_csv(\"Train.csv\")\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>location</th>\n",
              "      <th>temp</th>\n",
              "      <th>precip</th>\n",
              "      <th>rel_humidity</th>\n",
              "      <th>wind_dir</th>\n",
              "      <th>wind_spd</th>\n",
              "      <th>atmos_press</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ID_train_0</td>\n",
              "      <td>C</td>\n",
              "      <td>nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...</td>\n",
              "      <td>nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...</td>\n",
              "      <td>nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...</td>\n",
              "      <td>nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...</td>\n",
              "      <td>nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...</td>\n",
              "      <td>nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...</td>\n",
              "      <td>45.126304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ID_train_1</td>\n",
              "      <td>D</td>\n",
              "      <td>22.53333333,21.71666667,20.83333333,20.9833333...</td>\n",
              "      <td>0.102,0.0,0.0,0.0,0.0,0.0,0.0,0.034,0.017,0.01...</td>\n",
              "      <td>0.744583333,0.808083333,0.911166667,0.91633333...</td>\n",
              "      <td>281.6643101,89.15629262,81.96853891,291.018632...</td>\n",
              "      <td>2.3775,1.126666667,0.700833333,0.3416666670000...</td>\n",
              "      <td>90.32,90.3775,90.44083333,90.4725,90.45416667,...</td>\n",
              "      <td>79.131702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ID_train_10</td>\n",
              "      <td>A</td>\n",
              "      <td>28.975,27.95,29.6,26.425,22.09166667,21.775,22...</td>\n",
              "      <td>0.0,0.0,0.0,0.102,0.136,0.0,0.0,2.16,1.276,0.0...</td>\n",
              "      <td>0.573333333,0.597166667,0.5668333329999999,0.6...</td>\n",
              "      <td>nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...</td>\n",
              "      <td>nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...</td>\n",
              "      <td>88.55166667,88.46416667,88.31916667,88.24,88.2...</td>\n",
              "      <td>32.661304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ID_train_100</td>\n",
              "      <td>A</td>\n",
              "      <td>22.96666667,24.26666667,25.275,25.625,25.86666...</td>\n",
              "      <td>0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,7.77,3.012,1.0...</td>\n",
              "      <td>0.8430833329999999,0.79025,0.7375,0.728,0.7049...</td>\n",
              "      <td>300.0850574,293.6769595,294.5174647,301.921416...</td>\n",
              "      <td>1.446666667,1.1925,1.324166667,1.5441666669999...</td>\n",
              "      <td>88.615,88.53083333,88.4,88.27166667,88.2075,88...</td>\n",
              "      <td>53.850238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ID_train_1000</td>\n",
              "      <td>A</td>\n",
              "      <td>21.875,21.575,21.525,21.43333333,20.50833333,1...</td>\n",
              "      <td>0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....</td>\n",
              "      <td>0.8564166670000001,0.874916667,0.879833333,0.8...</td>\n",
              "      <td>21.83997432,17.05405341,89.26406044,123.585424...</td>\n",
              "      <td>0.1975,0.244166667,0.411666667,0.56,0.5775,0.4...</td>\n",
              "      <td>88.55666667,88.64083333,88.65833333,88.6475,88...</td>\n",
              "      <td>177.418750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15534</th>\n",
              "      <td>ID_train_9995</td>\n",
              "      <td>A</td>\n",
              "      <td>22.71666667,16.93333333,18.03333333,18.975,19....</td>\n",
              "      <td>31.467,31.842,0.34,0.0,0.0,0.119,0.0,0.017,0.0...</td>\n",
              "      <td>0.794090909,0.97425,0.997583333,0.9985,0.999,0...</td>\n",
              "      <td>nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...</td>\n",
              "      <td>nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...</td>\n",
              "      <td>88.27666667,88.24333333,88.18166667,88.1758333...</td>\n",
              "      <td>44.850286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15535</th>\n",
              "      <td>ID_train_9996</td>\n",
              "      <td>E</td>\n",
              "      <td>25.375,26.025,26.4,26.23333333,25.7,24.9,23.20...</td>\n",
              "      <td>0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....</td>\n",
              "      <td>0.685333333,0.6549166670000001,0.6435833329999...</td>\n",
              "      <td>176.5438256,196.5264893,177.1717795,151.268141...</td>\n",
              "      <td>1.328333333,1.278333333,1.535833333,2.315,1.92...</td>\n",
              "      <td>88.43666667,88.34583333,88.26083333,88.1741666...</td>\n",
              "      <td>24.330455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15536</th>\n",
              "      <td>ID_train_9997</td>\n",
              "      <td>D</td>\n",
              "      <td>26.09166667,28.975,30.31666667,30.96666667,30....</td>\n",
              "      <td>0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....</td>\n",
              "      <td>0.78175,0.5780833329999999,0.513333333,0.50233...</td>\n",
              "      <td>121.7060158,147.2472237,87.95942197,121.598728...</td>\n",
              "      <td>0.936666667,1.044166667,1.239166667,1.51083333...</td>\n",
              "      <td>90.61583333,90.44916667,90.32916667,90.27,90.2...</td>\n",
              "      <td>38.972128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15537</th>\n",
              "      <td>ID_train_9998</td>\n",
              "      <td>D</td>\n",
              "      <td>29.225,30.0,29.55,29.66666667,27.725,24.466666...</td>\n",
              "      <td>0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....</td>\n",
              "      <td>0.6283333329999999,0.605833333,0.594166667,0.5...</td>\n",
              "      <td>156.4757306,168.1802199,33.83625816,352.712136...</td>\n",
              "      <td>1.330833333,1.466666667,0.9075,1.060833333,0.8...</td>\n",
              "      <td>90.31416667,90.23916667,90.20166667,90.2225,90...</td>\n",
              "      <td>41.720952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15538</th>\n",
              "      <td>ID_train_9999</td>\n",
              "      <td>C</td>\n",
              "      <td>21.45,24.20833333,25.61666667,26.34166667,26.7...</td>\n",
              "      <td>0.017,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.187,0.0,0....</td>\n",
              "      <td>0.913333333,0.791666667,0.7125,0.6675,0.648333...</td>\n",
              "      <td>193.4763453,177.8990636,170.1562965,135.276065...</td>\n",
              "      <td>0.5,0.739166667,0.83,0.7958333329999999,0.7358...</td>\n",
              "      <td>88.0675,88.11666667,88.0925,88.03666667,87.952...</td>\n",
              "      <td>127.983333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15539 rows Ã— 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  ID  ...      target\n",
              "0         ID_train_0  ...   45.126304\n",
              "1         ID_train_1  ...   79.131702\n",
              "2        ID_train_10  ...   32.661304\n",
              "3       ID_train_100  ...   53.850238\n",
              "4      ID_train_1000  ...  177.418750\n",
              "...              ...  ...         ...\n",
              "15534  ID_train_9995  ...   44.850286\n",
              "15535  ID_train_9996  ...   24.330455\n",
              "15536  ID_train_9997  ...   38.972128\n",
              "15537  ID_train_9998  ...   41.720952\n",
              "15538  ID_train_9999  ...  127.983333\n",
              "\n",
              "[15539 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZUWe5fpDwzC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e06f0623-ccdb-4f51-fba3-c5ff733e7b19"
      },
      "source": [
        "df_locs = {}\n",
        "df_locs['A'] = df[df[\"location\"] == 'A']\n",
        "df_locs['B'] = df[df[\"location\"] == 'B']\n",
        "df_locs['C'] = df[df[\"location\"] == 'C']\n",
        "df_locs['D'] = df[df[\"location\"] == 'D']\n",
        "df_locs['E'] = df[df[\"location\"] == 'E']\n",
        "df_locs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'A':                    ID  ...      target\n",
              " 2         ID_train_10  ...   32.661304\n",
              " 3        ID_train_100  ...   53.850238\n",
              " 4       ID_train_1000  ...  177.418750\n",
              " 7      ID_train_10002  ...   55.051143\n",
              " 8      ID_train_10003  ...  205.215333\n",
              " ...               ...  ...         ...\n",
              " 15518   ID_train_9980  ...   44.962558\n",
              " 15519   ID_train_9981  ...  129.433636\n",
              " 15528    ID_train_999  ...   38.138182\n",
              " 15531   ID_train_9992  ...   24.302955\n",
              " 15534   ID_train_9995  ...   44.850286\n",
              " \n",
              " [5122 rows x 9 columns], 'B':                    ID  ...      target\n",
              " 9      ID_train_10004  ...   25.580541\n",
              " 53     ID_train_10044  ...   63.030385\n",
              " 88     ID_train_10076  ...   27.240250\n",
              " 92      ID_train_1008  ...   39.807949\n",
              " 128    ID_train_10111  ...    8.681000\n",
              " ...               ...  ...         ...\n",
              " 15349   ID_train_9828  ...   45.900732\n",
              " 15361   ID_train_9839  ...   42.629762\n",
              " 15364   ID_train_9841  ...  108.219024\n",
              " 15381   ID_train_9857  ...   50.928837\n",
              " 15494   ID_train_9959  ...   33.557619\n",
              " \n",
              " [767 rows x 9 columns], 'C':                    ID  ...      target\n",
              " 0          ID_train_0  ...   45.126304\n",
              " 6      ID_train_10001  ...   53.100000\n",
              " 12     ID_train_10007  ...   23.215250\n",
              " 21     ID_train_10015  ...   37.308333\n",
              " 38     ID_train_10030  ...   61.786522\n",
              " ...               ...  ...         ...\n",
              " 15459   ID_train_9927  ...   18.770000\n",
              " 15482   ID_train_9948  ...  143.871304\n",
              " 15498   ID_train_9962  ...   31.316957\n",
              " 15506    ID_train_997  ...   33.505217\n",
              " 15538   ID_train_9999  ...  127.983333\n",
              " \n",
              " [1753 rows x 9 columns], 'D':                    ID  ...     target\n",
              " 1          ID_train_1  ...  79.131702\n",
              " 10     ID_train_10005  ...  55.638261\n",
              " 13     ID_train_10008  ...  32.240851\n",
              " 15      ID_train_1001  ...  72.717021\n",
              " 22     ID_train_10016  ...  35.833571\n",
              " ...               ...  ...        ...\n",
              " 15529   ID_train_9990  ...  72.446739\n",
              " 15530   ID_train_9991  ...  40.978298\n",
              " 15532   ID_train_9993  ...  32.002766\n",
              " 15536   ID_train_9997  ...  38.972128\n",
              " 15537   ID_train_9998  ...  41.720952\n",
              " \n",
              " [4990 rows x 9 columns], 'E':                    ID  ...     target\n",
              " 5      ID_train_10000  ...  17.005000\n",
              " 11     ID_train_10006  ...  45.024500\n",
              " 20     ID_train_10014  ...  32.363333\n",
              " 29     ID_train_10022  ...  45.775714\n",
              " 30     ID_train_10023  ...  41.390455\n",
              " ...               ...  ...        ...\n",
              " 15497   ID_train_9961  ...  30.692069\n",
              " 15517    ID_train_998  ...  32.191944\n",
              " 15526   ID_train_9988  ...  33.834167\n",
              " 15533   ID_train_9994  ...  24.363043\n",
              " 15535   ID_train_9996  ...  24.330455\n",
              " \n",
              " [2907 rows x 9 columns]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9hQ8q6XEkJx",
        "colab_type": "text"
      },
      "source": [
        "Clean up dataframes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prlSBpZIZs17",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "3676b705-be56-4558-a4af-7dcb00fd2cf8"
      },
      "source": [
        "col = [\"temp\",\"precip\",\"rel_humidity\",\"wind_dir\",\"wind_spd\",\"atmos_press\"]\n",
        "munged_data = {}\n",
        "targets = {}\n",
        "for key in df_locs:\n",
        "  df_locs[key].drop(\"location\",axis=1, inplace=True)\n",
        "  df_locs[key].drop(\"ID\",axis=1,  inplace=True) \n",
        "  new_dfs= []\n",
        "  df2=df_locs[key]\n",
        "  for header in col:\n",
        "    temp_df = pd.DataFrame(df2[header].str.split(',').tolist())\n",
        "    for i, column in enumerate(list(temp_df.columns)):\n",
        "      temp_df[column] = pd.to_numeric(temp_df[column],errors='coerce')\n",
        "      temp_df.rename(columns={ temp_df.columns[i]: header+'_'+str(column) }, inplace = True)\n",
        "\n",
        "    new_dfs.append(temp_df)\n",
        "    df2.drop(header, axis=1, inplace=True)\n",
        "\n",
        "  tt = np.stack(new_dfs)\n",
        "  t_array = tt\n",
        "  t_array = t_array.transpose([1,2,0])\n",
        "  samples = []\n",
        "  tgts = []\n",
        "  for s in t_array:\n",
        "    working_df = pd.DataFrame(s)\n",
        "    working_df.dropna(inplace=True)\n",
        "    if len(working_df > 20):\n",
        "      for i in range(0,len(working_df)-21):\n",
        "        samples.append(working_df.iloc[i:i+20])\n",
        "        tgts.append(working_df.iloc[i+21]) \n",
        "  munged_data[key] = np.stack(samples)\n",
        "  targets[key] = np.stack(tgts)\n",
        "\n",
        "targets['A'].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(361359, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZhwcKqSLoiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYngvjqGigvN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "2ed375d8-6ac4-4d42-fa09-1638cd3fc129"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.BatchNormalization(input_shape=(20,6)))\n",
        "model.add(tf.keras.layers.LSTM(32))\n",
        "model.add(tf.keras.layers.Dense(6))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 20, 6)             24        \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                4992      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 5,214\n",
            "Trainable params: 5,202\n",
            "Non-trainable params: 12\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UGgmGkK8xzb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "871082e5-6a3b-4ba6-f035-d8df12510903"
      },
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
        "    rankdir='TB', expand_nested=True, dpi=96\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEx4KfSM9pHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "histories = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3pfAcaXi85i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c160c58b-8ccd-487e-8dcb-1dec12932839"
      },
      "source": [
        "to_train = 'E'\n",
        "\n",
        "models[to_train] = model\n",
        "data_train = munged_data[to_train][:(len(munged_data[to_train]) // 10) * 8]\n",
        "data_val = munged_data[to_train][(len(munged_data[to_train]) // 10) * 8:]\n",
        "\n",
        "target_train = targets[to_train][:(len(targets[to_train]) // 10) * 8]\n",
        "target_val = targets[to_train][(len(targets[to_train]) // 10) * 8:]\n",
        "((data_train.shape, target_train.shape),(data_val.shape, target_val.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(((215872, 20, 6), (215872, 6)), ((53973, 20, 6), (53973, 6)))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CV3rsWZ-JuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"training_E.ckpt\"\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtND1ou6izdM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "26719dcc-23c2-4d1b-b92f-cb332a2a2da9"
      },
      "source": [
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "models[to_train].compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mse')\n",
        "histories[to_train] = models[to_train].fit(data_train, target_train, epochs=1000, callbacks=[early_stop, cp_callback], validation_data=(data_val, target_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "6740/6746 [============================>.] - ETA: 0s - loss: 4040.8604\n",
            "Epoch 00001: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 51s 8ms/step - loss: 4039.1282 - val_loss: 2165.4612\n",
            "Epoch 2/1000\n",
            "6746/6746 [==============================] - ETA: 0s - loss: 1976.1528\n",
            "Epoch 00002: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 1976.1528 - val_loss: 1759.3439\n",
            "Epoch 3/1000\n",
            "6740/6746 [============================>.] - ETA: 0s - loss: 1640.3080\n",
            "Epoch 00003: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 1640.2250 - val_loss: 1458.8143\n",
            "Epoch 4/1000\n",
            "6742/6746 [============================>.] - ETA: 0s - loss: 1420.4478\n",
            "Epoch 00004: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 1420.4276 - val_loss: 1238.7788\n",
            "Epoch 5/1000\n",
            "6742/6746 [============================>.] - ETA: 0s - loss: 1268.4965\n",
            "Epoch 00005: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 1268.6313 - val_loss: 1144.6211\n",
            "Epoch 6/1000\n",
            "6746/6746 [==============================] - ETA: 0s - loss: 1169.5258\n",
            "Epoch 00006: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 1169.5258 - val_loss: 1028.3541\n",
            "Epoch 7/1000\n",
            "6744/6746 [============================>.] - ETA: 0s - loss: 1078.3640\n",
            "Epoch 00007: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 1078.4105 - val_loss: 922.1607\n",
            "Epoch 8/1000\n",
            "6746/6746 [==============================] - ETA: 0s - loss: 993.1138\n",
            "Epoch 00008: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 993.1138 - val_loss: 819.9475\n",
            "Epoch 9/1000\n",
            "6745/6746 [============================>.] - ETA: 0s - loss: 927.0504\n",
            "Epoch 00009: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 927.0065 - val_loss: 781.8867\n",
            "Epoch 10/1000\n",
            "6746/6746 [==============================] - ETA: 0s - loss: 874.7079\n",
            "Epoch 00010: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 874.7079 - val_loss: 698.3896\n",
            "Epoch 11/1000\n",
            "6740/6746 [============================>.] - ETA: 0s - loss: 822.8760\n",
            "Epoch 00011: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 822.9423 - val_loss: 630.6805\n",
            "Epoch 12/1000\n",
            "6742/6746 [============================>.] - ETA: 0s - loss: 779.6713\n",
            "Epoch 00012: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 779.6811 - val_loss: 606.8831\n",
            "Epoch 13/1000\n",
            "6746/6746 [==============================] - ETA: 0s - loss: 738.7313\n",
            "Epoch 00013: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 738.7313 - val_loss: 591.9314\n",
            "Epoch 14/1000\n",
            "6745/6746 [============================>.] - ETA: 0s - loss: 713.5218\n",
            "Epoch 00014: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 713.5632 - val_loss: 535.7706\n",
            "Epoch 15/1000\n",
            "6739/6746 [============================>.] - ETA: 0s - loss: 681.7106\n",
            "Epoch 00015: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 681.6171 - val_loss: 512.6148\n",
            "Epoch 16/1000\n",
            "6744/6746 [============================>.] - ETA: 0s - loss: 655.3289\n",
            "Epoch 00016: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 48s 7ms/step - loss: 655.5464 - val_loss: 521.6340\n",
            "Epoch 17/1000\n",
            "6746/6746 [==============================] - ETA: 0s - loss: 630.6076\n",
            "Epoch 00017: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 630.6076 - val_loss: 499.5648\n",
            "Epoch 18/1000\n",
            "6745/6746 [============================>.] - ETA: 0s - loss: 614.7874\n",
            "Epoch 00018: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 614.7890 - val_loss: 491.0674\n",
            "Epoch 19/1000\n",
            "6743/6746 [============================>.] - ETA: 0s - loss: 590.2145\n",
            "Epoch 00019: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 590.3620 - val_loss: 424.5507\n",
            "Epoch 20/1000\n",
            "6746/6746 [==============================] - ETA: 0s - loss: 574.0796\n",
            "Epoch 00020: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 574.0796 - val_loss: 391.6589\n",
            "Epoch 21/1000\n",
            "6738/6746 [============================>.] - ETA: 0s - loss: 546.5704\n",
            "Epoch 00021: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 546.6486 - val_loss: 367.6821\n",
            "Epoch 22/1000\n",
            "6740/6746 [============================>.] - ETA: 0s - loss: 529.5117\n",
            "Epoch 00022: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 529.5168 - val_loss: 356.6085\n",
            "Epoch 23/1000\n",
            "6744/6746 [============================>.] - ETA: 0s - loss: 526.8780\n",
            "Epoch 00023: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 526.9719 - val_loss: 370.7059\n",
            "Epoch 24/1000\n",
            "6745/6746 [============================>.] - ETA: 0s - loss: 513.0536\n",
            "Epoch 00024: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 513.0485 - val_loss: 327.8965\n",
            "Epoch 25/1000\n",
            "6744/6746 [============================>.] - ETA: 0s - loss: 492.9056\n",
            "Epoch 00025: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 492.8307 - val_loss: 336.3155\n",
            "Epoch 26/1000\n",
            "6740/6746 [============================>.] - ETA: 0s - loss: 480.2121\n",
            "Epoch 00026: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 480.1944 - val_loss: 318.0009\n",
            "Epoch 27/1000\n",
            "6742/6746 [============================>.] - ETA: 0s - loss: 469.3425\n",
            "Epoch 00027: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 469.4535 - val_loss: 309.0445\n",
            "Epoch 28/1000\n",
            "6742/6746 [============================>.] - ETA: 0s - loss: 453.9816\n",
            "Epoch 00028: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 453.9822 - val_loss: 284.9701\n",
            "Epoch 29/1000\n",
            "6739/6746 [============================>.] - ETA: 0s - loss: 445.1605\n",
            "Epoch 00029: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 445.1741 - val_loss: 300.0105\n",
            "Epoch 30/1000\n",
            "6741/6746 [============================>.] - ETA: 0s - loss: 428.7970\n",
            "Epoch 00030: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 48s 7ms/step - loss: 428.8476 - val_loss: 275.6706\n",
            "Epoch 31/1000\n",
            "6745/6746 [============================>.] - ETA: 0s - loss: 429.3437\n",
            "Epoch 00031: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 429.3195 - val_loss: 287.7791\n",
            "Epoch 32/1000\n",
            "6744/6746 [============================>.] - ETA: 0s - loss: 417.3619\n",
            "Epoch 00032: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 417.3236 - val_loss: 269.6703\n",
            "Epoch 33/1000\n",
            "6740/6746 [============================>.] - ETA: 0s - loss: 412.5191\n",
            "Epoch 00033: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 49s 7ms/step - loss: 412.4767 - val_loss: 244.7001\n",
            "Epoch 34/1000\n",
            "6745/6746 [============================>.] - ETA: 0s - loss: 412.2637\n",
            "Epoch 00034: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 50s 7ms/step - loss: 412.2497 - val_loss: 240.2184\n",
            "Epoch 35/1000\n",
            "6745/6746 [============================>.] - ETA: 0s - loss: 399.0813\n",
            "Epoch 00035: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 50s 7ms/step - loss: 399.0386 - val_loss: 261.9533\n",
            "Epoch 36/1000\n",
            "6740/6746 [============================>.] - ETA: 0s - loss: 395.5176\n",
            "Epoch 00036: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 51s 8ms/step - loss: 395.6558 - val_loss: 242.0073\n",
            "Epoch 37/1000\n",
            "6746/6746 [==============================] - ETA: 0s - loss: 381.3060\n",
            "Epoch 00037: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 49s 7ms/step - loss: 381.3060 - val_loss: 229.3250\n",
            "Epoch 38/1000\n",
            "6745/6746 [============================>.] - ETA: 0s - loss: 381.4067\n",
            "Epoch 00038: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 50s 7ms/step - loss: 381.3896 - val_loss: 229.4263\n",
            "Epoch 39/1000\n",
            "6743/6746 [============================>.] - ETA: 0s - loss: 376.5324\n",
            "Epoch 00039: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 376.4893 - val_loss: 236.0654\n",
            "Epoch 40/1000\n",
            "6742/6746 [============================>.] - ETA: 0s - loss: 368.9071\n",
            "Epoch 00040: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 50s 7ms/step - loss: 368.8738 - val_loss: 217.3072\n",
            "Epoch 41/1000\n",
            "6741/6746 [============================>.] - ETA: 0s - loss: 367.4949\n",
            "Epoch 00041: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 367.6097 - val_loss: 223.3555\n",
            "Epoch 42/1000\n",
            "6742/6746 [============================>.] - ETA: 0s - loss: 363.0553\n",
            "Epoch 00042: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 363.1762 - val_loss: 238.8466\n",
            "Epoch 43/1000\n",
            "6746/6746 [==============================] - ETA: 0s - loss: 363.0059\n",
            "Epoch 00043: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 363.0059 - val_loss: 213.1485\n",
            "Epoch 44/1000\n",
            "6739/6746 [============================>.] - ETA: 0s - loss: 351.6715\n",
            "Epoch 00044: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 351.6262 - val_loss: 196.4269\n",
            "Epoch 45/1000\n",
            "6743/6746 [============================>.] - ETA: 0s - loss: 354.4744\n",
            "Epoch 00045: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 49s 7ms/step - loss: 354.4393 - val_loss: 198.5220\n",
            "Epoch 46/1000\n",
            "6745/6746 [============================>.] - ETA: 0s - loss: 346.7435\n",
            "Epoch 00046: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 346.7436 - val_loss: 205.4696\n",
            "Epoch 47/1000\n",
            "6745/6746 [============================>.] - ETA: 0s - loss: 342.3775\n",
            "Epoch 00047: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 342.3574 - val_loss: 198.0149\n",
            "Epoch 48/1000\n",
            "6744/6746 [============================>.] - ETA: 0s - loss: 342.7045\n",
            "Epoch 00048: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 342.7373 - val_loss: 188.9758\n",
            "Epoch 49/1000\n",
            "6741/6746 [============================>.] - ETA: 0s - loss: 338.6831\n",
            "Epoch 00049: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 338.5985 - val_loss: 193.2730\n",
            "Epoch 50/1000\n",
            "6742/6746 [============================>.] - ETA: 0s - loss: 338.0436\n",
            "Epoch 00050: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 338.1380 - val_loss: 190.6576\n",
            "Epoch 51/1000\n",
            "6746/6746 [==============================] - ETA: 0s - loss: 330.6485\n",
            "Epoch 00051: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 330.6485 - val_loss: 205.1787\n",
            "Epoch 52/1000\n",
            "6744/6746 [============================>.] - ETA: 0s - loss: 327.7773\n",
            "Epoch 00052: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 327.9012 - val_loss: 210.7135\n",
            "Epoch 53/1000\n",
            "6746/6746 [==============================] - ETA: 0s - loss: 331.8467\n",
            "Epoch 00053: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 331.8467 - val_loss: 191.0489\n",
            "Epoch 54/1000\n",
            "6739/6746 [============================>.] - ETA: 0s - loss: 328.8859\n",
            "Epoch 00054: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 328.7927 - val_loss: 177.8541\n",
            "Epoch 55/1000\n",
            "6743/6746 [============================>.] - ETA: 0s - loss: 316.3801\n",
            "Epoch 00055: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 316.3425 - val_loss: 197.1536\n",
            "Epoch 56/1000\n",
            "6746/6746 [==============================] - ETA: 0s - loss: 328.3151\n",
            "Epoch 00056: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 328.3151 - val_loss: 183.3015\n",
            "Epoch 57/1000\n",
            "6739/6746 [============================>.] - ETA: 0s - loss: 322.7734\n",
            "Epoch 00057: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 322.7273 - val_loss: 200.6322\n",
            "Epoch 58/1000\n",
            "6742/6746 [============================>.] - ETA: 0s - loss: 314.5722\n",
            "Epoch 00058: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 314.7736 - val_loss: 182.3210\n",
            "Epoch 59/1000\n",
            "6743/6746 [============================>.] - ETA: 0s - loss: 315.3415\n",
            "Epoch 00059: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 48s 7ms/step - loss: 315.3549 - val_loss: 184.7507\n",
            "Epoch 60/1000\n",
            "6746/6746 [==============================] - ETA: 0s - loss: 309.9472\n",
            "Epoch 00060: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 49s 7ms/step - loss: 309.9472 - val_loss: 187.8491\n",
            "Epoch 61/1000\n",
            "6744/6746 [============================>.] - ETA: 0s - loss: 305.6990\n",
            "Epoch 00061: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 305.7204 - val_loss: 166.8877\n",
            "Epoch 62/1000\n",
            "6746/6746 [==============================] - ETA: 0s - loss: 303.7933\n",
            "Epoch 00062: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 49s 7ms/step - loss: 303.7933 - val_loss: 158.5028\n",
            "Epoch 63/1000\n",
            "6745/6746 [============================>.] - ETA: 0s - loss: 299.9646\n",
            "Epoch 00063: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 299.9544 - val_loss: 159.5022\n",
            "Epoch 64/1000\n",
            "6745/6746 [============================>.] - ETA: 0s - loss: 305.6609\n",
            "Epoch 00064: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 48s 7ms/step - loss: 305.6406 - val_loss: 162.8407\n",
            "Epoch 65/1000\n",
            "6742/6746 [============================>.] - ETA: 0s - loss: 303.9876\n",
            "Epoch 00065: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 303.9365 - val_loss: 159.5101\n",
            "Epoch 66/1000\n",
            "6741/6746 [============================>.] - ETA: 0s - loss: 297.5165\n",
            "Epoch 00066: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 297.4927 - val_loss: 165.4906\n",
            "Epoch 67/1000\n",
            "6745/6746 [============================>.] - ETA: 0s - loss: 297.1179\n",
            "Epoch 00067: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 297.1805 - val_loss: 168.3438\n",
            "Epoch 68/1000\n",
            "6743/6746 [============================>.] - ETA: 0s - loss: 295.6569\n",
            "Epoch 00068: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 295.6534 - val_loss: 155.4183\n",
            "Epoch 69/1000\n",
            "6740/6746 [============================>.] - ETA: 0s - loss: 293.3393\n",
            "Epoch 00069: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 293.2931 - val_loss: 159.3548\n",
            "Epoch 70/1000\n",
            "6744/6746 [============================>.] - ETA: 0s - loss: 296.5523\n",
            "Epoch 00070: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 49s 7ms/step - loss: 296.5544 - val_loss: 147.0396\n",
            "Epoch 71/1000\n",
            "6740/6746 [============================>.] - ETA: 0s - loss: 288.8254\n",
            "Epoch 00071: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 288.7480 - val_loss: 167.8824\n",
            "Epoch 72/1000\n",
            "6742/6746 [============================>.] - ETA: 0s - loss: 285.2327\n",
            "Epoch 00072: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 285.2899 - val_loss: 151.4271\n",
            "Epoch 73/1000\n",
            "6739/6746 [============================>.] - ETA: 0s - loss: 289.9761\n",
            "Epoch 00073: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 290.1216 - val_loss: 167.5569\n",
            "Epoch 74/1000\n",
            "6741/6746 [============================>.] - ETA: 0s - loss: 287.8727\n",
            "Epoch 00074: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 287.7788 - val_loss: 139.1292\n",
            "Epoch 75/1000\n",
            "6741/6746 [============================>.] - ETA: 0s - loss: 287.1851\n",
            "Epoch 00075: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 48s 7ms/step - loss: 287.1129 - val_loss: 159.5563\n",
            "Epoch 76/1000\n",
            "6740/6746 [============================>.] - ETA: 0s - loss: 289.3592\n",
            "Epoch 00076: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 289.2260 - val_loss: 157.5174\n",
            "Epoch 77/1000\n",
            "6740/6746 [============================>.] - ETA: 0s - loss: 286.2973\n",
            "Epoch 00077: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 286.3125 - val_loss: 141.5637\n",
            "Epoch 78/1000\n",
            "6745/6746 [============================>.] - ETA: 0s - loss: 279.3815\n",
            "Epoch 00078: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 279.3770 - val_loss: 142.8068\n",
            "Epoch 79/1000\n",
            "6745/6746 [============================>.] - ETA: 0s - loss: 278.7911\n",
            "Epoch 00079: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 46s 7ms/step - loss: 278.7640 - val_loss: 153.7578\n",
            "Epoch 80/1000\n",
            "6746/6746 [==============================] - ETA: 0s - loss: 285.5222\n",
            "Epoch 00080: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 285.5222 - val_loss: 155.4989\n",
            "Epoch 81/1000\n",
            "6745/6746 [============================>.] - ETA: 0s - loss: 279.8636\n",
            "Epoch 00081: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 279.8643 - val_loss: 159.9300\n",
            "Epoch 82/1000\n",
            "6746/6746 [==============================] - ETA: 0s - loss: 280.9464\n",
            "Epoch 00082: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 280.9464 - val_loss: 146.7045\n",
            "Epoch 83/1000\n",
            "6746/6746 [==============================] - ETA: 0s - loss: 274.2129\n",
            "Epoch 00083: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 274.2129 - val_loss: 161.3535\n",
            "Epoch 84/1000\n",
            "6745/6746 [============================>.] - ETA: 0s - loss: 280.9555\n",
            "Epoch 00084: saving model to training_E.ckpt\n",
            "6746/6746 [==============================] - 47s 7ms/step - loss: 280.9268 - val_loss: 146.8898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12Sk0fCJn-zx",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpK4h5Bdsh7z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5659f0ab-a62b-4991-8977-3655901ae53b"
      },
      "source": [
        "(models['E'].predict(data_val)[0], target_val[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([21.451345  ,  0.20424089,  0.8056728 , 79.364586  ,  1.4305003 ,\n",
              "        88.77622   ], dtype=float32),\n",
              " array([2.22583333e+01, 1.70000000e-02, 8.29000000e-01, 3.89159780e+01,\n",
              "        1.16916667e+00, 8.83883333e+01]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSDniDzct_F9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "354ae2d0-bafe-4c7b-e285-77cfeef3699a"
      },
      "source": [
        "new_model = tf.keras.models.Sequential()\n",
        "new_model.add(tf.keras.layers.BatchNormalization(input_shape=(20,6)))\n",
        "new_model.add(tf.keras.layers.LSTM(32))\n",
        "new_model.add(tf.keras.layers.Dense(6))\n",
        "new_model.load_weights(\"training_E.ckpt\")\n",
        "(new_model.predict(data_val)[0], target_val[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([21.451345  ,  0.20424089,  0.8056728 , 79.364586  ,  1.4305003 ,\n",
              "        88.77622   ], dtype=float32),\n",
              " array([2.22583333e+01, 1.70000000e-02, 8.29000000e-01, 3.89159780e+01,\n",
              "        1.16916667e+00, 8.83883333e+01]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snndvd7qLt7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models['A'] = model\n",
        "histories = {}\n",
        "histories['A'] = history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgM5VuChM1sc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "55a11798-dc86-4752-b333-2ca2c72a40f1"
      },
      "source": [
        "munged_data['E'].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(269845, 20, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek-Mykr8LzCu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cdcbcfc8-5c2a-4dbb-e010-cf0a61aea228"
      },
      "source": [
        "for key in ['B','C','D','E']:\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.BatchNormalization(input_shape=(20,6)))\n",
        "  model.add(tf.keras.layers.LSTM(32))\n",
        "  model.add(tf.keras.layers.Dense(6))\n",
        "\n",
        "  data_train = munged_data[key][:(len(munged_data[key]) // 10) * 8]\n",
        "  data_val = munged_data[key][(len(munged_data[key]) // 10) * 8:]\n",
        "  target_train = targets[key][:(len(targets[key]) // 10) * 8]\n",
        "  target_val = targets[key][(len(targets[key]) // 10) * 8:]\n",
        "  early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "  model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mse')\n",
        "  history = model.fit(data_train, target_train, epochs=1000, callbacks=[early_stop], validation_data=(data_val, target_val))\n",
        "  models[key] = model\n",
        "  histories[key] = history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "1661/1661 [==============================] - 12s 7ms/step - loss: 8478.6631 - val_loss: 6648.1421\n",
            "Epoch 2/1000\n",
            "1661/1661 [==============================] - 12s 7ms/step - loss: 4892.6309 - val_loss: 3927.4429\n",
            "Epoch 3/1000\n",
            "1661/1661 [==============================] - 12s 7ms/step - loss: 2893.1135 - val_loss: 2375.5352\n",
            "Epoch 4/1000\n",
            "1661/1661 [==============================] - 13s 8ms/step - loss: 1908.0098 - val_loss: 1796.9633\n",
            "Epoch 5/1000\n",
            "1661/1661 [==============================] - 12s 7ms/step - loss: 1708.7113 - val_loss: 1759.3126\n",
            "Epoch 6/1000\n",
            "1661/1661 [==============================] - 12s 7ms/step - loss: 1704.8676 - val_loss: 1757.4417\n",
            "Epoch 7/1000\n",
            "1661/1661 [==============================] - 12s 7ms/step - loss: 1705.0071 - val_loss: 1758.5652\n",
            "Epoch 8/1000\n",
            "1661/1661 [==============================] - 12s 7ms/step - loss: 1705.0105 - val_loss: 1758.2451\n",
            "Epoch 9/1000\n",
            "1661/1661 [==============================] - 12s 7ms/step - loss: 1704.9858 - val_loss: 1759.0718\n",
            "Epoch 10/1000\n",
            "1661/1661 [==============================] - 12s 7ms/step - loss: 1704.9891 - val_loss: 1758.8953\n",
            "Epoch 11/1000\n",
            "1661/1661 [==============================] - 12s 7ms/step - loss: 1704.9929 - val_loss: 1758.1888\n",
            "Epoch 12/1000\n",
            "1661/1661 [==============================] - 12s 7ms/step - loss: 1704.9905 - val_loss: 1758.3287\n",
            "Epoch 13/1000\n",
            "1661/1661 [==============================] - 12s 7ms/step - loss: 1705.0122 - val_loss: 1758.8314\n",
            "Epoch 14/1000\n",
            "1661/1661 [==============================] - 12s 7ms/step - loss: 1705.0065 - val_loss: 1757.9857\n",
            "Epoch 15/1000\n",
            "1661/1661 [==============================] - 12s 7ms/step - loss: 1704.9847 - val_loss: 1759.0641\n",
            "Epoch 16/1000\n",
            "1661/1661 [==============================] - 12s 7ms/step - loss: 1705.0222 - val_loss: 1758.9271\n",
            "Epoch 1/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 5059.2930 - val_loss: 2168.7012\n",
            "Epoch 2/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 1435.8464 - val_loss: 1292.5618\n",
            "Epoch 3/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 1289.3805 - val_loss: 1292.2313\n",
            "Epoch 4/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 1289.4031 - val_loss: 1292.4633\n",
            "Epoch 5/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 1140.0728 - val_loss: 975.7261\n",
            "Epoch 6/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 877.6276 - val_loss: 779.1843\n",
            "Epoch 7/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 741.1396 - val_loss: 675.8837\n",
            "Epoch 8/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 644.3253 - val_loss: 565.4493\n",
            "Epoch 9/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 579.6189 - val_loss: 544.4895\n",
            "Epoch 10/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 534.0736 - val_loss: 473.0923\n",
            "Epoch 11/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 492.4215 - val_loss: 422.1202\n",
            "Epoch 12/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 463.4893 - val_loss: 380.2108\n",
            "Epoch 13/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 436.0958 - val_loss: 383.5287\n",
            "Epoch 14/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 414.5239 - val_loss: 354.6165\n",
            "Epoch 15/1000\n",
            "4246/4246 [==============================] - 41s 10ms/step - loss: 398.7697 - val_loss: 376.1522\n",
            "Epoch 16/1000\n",
            "4246/4246 [==============================] - 41s 10ms/step - loss: 378.9565 - val_loss: 301.1765\n",
            "Epoch 17/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 366.1150 - val_loss: 284.7375\n",
            "Epoch 18/1000\n",
            "4246/4246 [==============================] - 41s 10ms/step - loss: 352.6480 - val_loss: 292.0623\n",
            "Epoch 19/1000\n",
            "4246/4246 [==============================] - 41s 10ms/step - loss: 343.9578 - val_loss: 293.7589\n",
            "Epoch 20/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 332.6975 - val_loss: 259.1331\n",
            "Epoch 21/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 321.8544 - val_loss: 255.2290\n",
            "Epoch 22/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 316.7379 - val_loss: 245.6563\n",
            "Epoch 23/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 305.7917 - val_loss: 252.4893\n",
            "Epoch 24/1000\n",
            "4246/4246 [==============================] - 41s 10ms/step - loss: 300.6867 - val_loss: 248.1292\n",
            "Epoch 25/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 297.1741 - val_loss: 224.7441\n",
            "Epoch 26/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 288.6239 - val_loss: 222.3314\n",
            "Epoch 27/1000\n",
            "4246/4246 [==============================] - 41s 10ms/step - loss: 280.9366 - val_loss: 231.2034\n",
            "Epoch 28/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 275.2393 - val_loss: 215.8699\n",
            "Epoch 29/1000\n",
            "4246/4246 [==============================] - 41s 10ms/step - loss: 269.1605 - val_loss: 200.4118\n",
            "Epoch 30/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 263.6934 - val_loss: 199.7551\n",
            "Epoch 31/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 256.9249 - val_loss: 216.4697\n",
            "Epoch 32/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 255.9633 - val_loss: 191.2445\n",
            "Epoch 33/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 246.5373 - val_loss: 197.3408\n",
            "Epoch 34/1000\n",
            "4246/4246 [==============================] - 41s 10ms/step - loss: 243.4080 - val_loss: 188.9311\n",
            "Epoch 35/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 239.9513 - val_loss: 194.3198\n",
            "Epoch 36/1000\n",
            "4246/4246 [==============================] - 41s 10ms/step - loss: 236.8951 - val_loss: 183.1392\n",
            "Epoch 37/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 234.9910 - val_loss: 169.5535\n",
            "Epoch 38/1000\n",
            "4246/4246 [==============================] - 41s 10ms/step - loss: 229.4375 - val_loss: 170.9269\n",
            "Epoch 39/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 226.2263 - val_loss: 169.5445\n",
            "Epoch 40/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 224.1080 - val_loss: 166.2425\n",
            "Epoch 41/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 221.7263 - val_loss: 165.0244\n",
            "Epoch 42/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 216.9919 - val_loss: 157.7987\n",
            "Epoch 43/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 216.1653 - val_loss: 161.4633\n",
            "Epoch 44/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 213.7531 - val_loss: 170.5020\n",
            "Epoch 45/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 211.7705 - val_loss: 158.9865\n",
            "Epoch 46/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 206.2560 - val_loss: 154.8895\n",
            "Epoch 47/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 207.0274 - val_loss: 155.6795\n",
            "Epoch 48/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 205.2224 - val_loss: 152.6819\n",
            "Epoch 49/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 199.7593 - val_loss: 167.0765\n",
            "Epoch 50/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 201.4615 - val_loss: 151.2826\n",
            "Epoch 51/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 198.1214 - val_loss: 153.2996\n",
            "Epoch 52/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 194.2508 - val_loss: 139.6478\n",
            "Epoch 53/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 195.7191 - val_loss: 136.6919\n",
            "Epoch 54/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 195.0816 - val_loss: 143.2710\n",
            "Epoch 55/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 192.0986 - val_loss: 149.6848\n",
            "Epoch 56/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 190.5271 - val_loss: 132.5473\n",
            "Epoch 57/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 190.3308 - val_loss: 161.3811\n",
            "Epoch 58/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 188.7601 - val_loss: 127.8512\n",
            "Epoch 59/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 185.9678 - val_loss: 136.5275\n",
            "Epoch 60/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 186.5834 - val_loss: 131.2862\n",
            "Epoch 61/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 185.4204 - val_loss: 137.1318\n",
            "Epoch 62/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 182.6800 - val_loss: 125.2140\n",
            "Epoch 63/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 182.3894 - val_loss: 126.8052\n",
            "Epoch 64/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 182.1852 - val_loss: 128.6158\n",
            "Epoch 65/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 181.5206 - val_loss: 129.9660\n",
            "Epoch 66/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 180.2962 - val_loss: 134.6308\n",
            "Epoch 67/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 177.3086 - val_loss: 124.0353\n",
            "Epoch 68/1000\n",
            "4246/4246 [==============================] - 41s 10ms/step - loss: 178.1974 - val_loss: 126.7632\n",
            "Epoch 69/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 177.7123 - val_loss: 120.7776\n",
            "Epoch 70/1000\n",
            "4246/4246 [==============================] - 41s 10ms/step - loss: 176.0592 - val_loss: 128.6752\n",
            "Epoch 71/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 173.0567 - val_loss: 119.7546\n",
            "Epoch 72/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 171.5950 - val_loss: 118.3491\n",
            "Epoch 73/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 170.9115 - val_loss: 114.0724\n",
            "Epoch 74/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 172.6218 - val_loss: 120.5361\n",
            "Epoch 75/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 171.1606 - val_loss: 120.1688\n",
            "Epoch 76/1000\n",
            "4246/4246 [==============================] - 41s 10ms/step - loss: 169.1189 - val_loss: 114.7383\n",
            "Epoch 77/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 166.1636 - val_loss: 117.0053\n",
            "Epoch 78/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 167.0752 - val_loss: 105.9152\n",
            "Epoch 79/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 164.4014 - val_loss: 105.4236\n",
            "Epoch 80/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 163.4303 - val_loss: 115.5212\n",
            "Epoch 81/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 165.9757 - val_loss: 108.9368\n",
            "Epoch 82/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 168.7056 - val_loss: 113.4410\n",
            "Epoch 83/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 162.5125 - val_loss: 115.9984\n",
            "Epoch 84/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 162.3029 - val_loss: 105.9330\n",
            "Epoch 85/1000\n",
            "4246/4246 [==============================] - 41s 10ms/step - loss: 163.3212 - val_loss: 118.2645\n",
            "Epoch 86/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 161.9832 - val_loss: 104.8243\n",
            "Epoch 87/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 156.8199 - val_loss: 105.6708\n",
            "Epoch 88/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 159.3146 - val_loss: 107.3854\n",
            "Epoch 89/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 157.5405 - val_loss: 112.3745\n",
            "Epoch 90/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 157.9693 - val_loss: 97.4647\n",
            "Epoch 91/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 158.4963 - val_loss: 99.8502\n",
            "Epoch 92/1000\n",
            "4246/4246 [==============================] - 41s 10ms/step - loss: 156.9288 - val_loss: 106.5630\n",
            "Epoch 93/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 153.9722 - val_loss: 105.0629\n",
            "Epoch 94/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 155.0968 - val_loss: 110.7328\n",
            "Epoch 95/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 153.7462 - val_loss: 96.3091\n",
            "Epoch 96/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 153.1205 - val_loss: 110.5774\n",
            "Epoch 97/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 152.7079 - val_loss: 104.8252\n",
            "Epoch 98/1000\n",
            "4246/4246 [==============================] - 44s 10ms/step - loss: 155.5995 - val_loss: 100.8857\n",
            "Epoch 99/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 153.5982 - val_loss: 108.4969\n",
            "Epoch 100/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 154.6933 - val_loss: 90.8947\n",
            "Epoch 101/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 152.1912 - val_loss: 96.3871\n",
            "Epoch 102/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 151.2444 - val_loss: 105.8212\n",
            "Epoch 103/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 152.0981 - val_loss: 102.9349\n",
            "Epoch 104/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 147.9343 - val_loss: 99.8920\n",
            "Epoch 105/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 147.6356 - val_loss: 107.5988\n",
            "Epoch 106/1000\n",
            "4246/4246 [==============================] - 43s 10ms/step - loss: 147.9748 - val_loss: 100.9446\n",
            "Epoch 107/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 148.3500 - val_loss: 95.7074\n",
            "Epoch 108/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 149.8255 - val_loss: 95.8443\n",
            "Epoch 109/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 147.9301 - val_loss: 102.8822\n",
            "Epoch 110/1000\n",
            "4246/4246 [==============================] - 42s 10ms/step - loss: 145.6016 - val_loss: 98.2836\n",
            "Epoch 1/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 2621.7783 - val_loss: 1559.5291\n",
            "Epoch 2/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 1499.1443 - val_loss: 1435.9045\n",
            "Epoch 3/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 1396.9558 - val_loss: 1332.6790\n",
            "Epoch 4/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 1311.9192 - val_loss: 1264.2379\n",
            "Epoch 5/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 1215.4171 - val_loss: 1124.5457\n",
            "Epoch 6/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 1128.3330 - val_loss: 1021.0991\n",
            "Epoch 7/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 1062.1699 - val_loss: 926.6074\n",
            "Epoch 8/1000\n",
            "12062/12062 [==============================] - 121s 10ms/step - loss: 1003.7354 - val_loss: 906.2452\n",
            "Epoch 9/1000\n",
            "12062/12062 [==============================] - 118s 10ms/step - loss: 955.4915 - val_loss: 813.1235\n",
            "Epoch 10/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 912.8611 - val_loss: 773.3517\n",
            "Epoch 11/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 885.7190 - val_loss: 708.3570\n",
            "Epoch 12/1000\n",
            "12062/12062 [==============================] - 118s 10ms/step - loss: 854.5300 - val_loss: 730.1229\n",
            "Epoch 13/1000\n",
            "12062/12062 [==============================] - 121s 10ms/step - loss: 824.0051 - val_loss: 665.9774\n",
            "Epoch 14/1000\n",
            "12062/12062 [==============================] - 123s 10ms/step - loss: 804.1917 - val_loss: 667.1922\n",
            "Epoch 15/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 786.6785 - val_loss: 618.5697\n",
            "Epoch 16/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 772.4702 - val_loss: 627.3046\n",
            "Epoch 17/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 754.8478 - val_loss: 636.3699\n",
            "Epoch 18/1000\n",
            "12062/12062 [==============================] - 121s 10ms/step - loss: 745.2251 - val_loss: 580.2767\n",
            "Epoch 19/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 734.1324 - val_loss: 580.2491\n",
            "Epoch 20/1000\n",
            "12062/12062 [==============================] - 118s 10ms/step - loss: 720.8996 - val_loss: 563.0330\n",
            "Epoch 21/1000\n",
            "12062/12062 [==============================] - 121s 10ms/step - loss: 712.8909 - val_loss: 546.7958\n",
            "Epoch 22/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 706.1663 - val_loss: 554.9340\n",
            "Epoch 23/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 695.2814 - val_loss: 518.5836\n",
            "Epoch 24/1000\n",
            "12062/12062 [==============================] - 121s 10ms/step - loss: 689.5035 - val_loss: 527.1609\n",
            "Epoch 25/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 686.5032 - val_loss: 549.3294\n",
            "Epoch 26/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 678.2798 - val_loss: 522.5666\n",
            "Epoch 27/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 671.7896 - val_loss: 507.9664\n",
            "Epoch 28/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 665.3608 - val_loss: 502.8076\n",
            "Epoch 29/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 661.4030 - val_loss: 491.7431\n",
            "Epoch 30/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 654.0128 - val_loss: 485.2075\n",
            "Epoch 31/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 647.6666 - val_loss: 497.7677\n",
            "Epoch 32/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 644.8835 - val_loss: 465.7790\n",
            "Epoch 33/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 642.4697 - val_loss: 475.1278\n",
            "Epoch 34/1000\n",
            "12062/12062 [==============================] - 121s 10ms/step - loss: 638.9843 - val_loss: 473.1253\n",
            "Epoch 35/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 633.2338 - val_loss: 485.2050\n",
            "Epoch 36/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 627.6976 - val_loss: 459.0500\n",
            "Epoch 37/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 624.4227 - val_loss: 440.7953\n",
            "Epoch 38/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 625.9822 - val_loss: 446.9250\n",
            "Epoch 39/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 621.4516 - val_loss: 454.3400\n",
            "Epoch 40/1000\n",
            "12062/12062 [==============================] - 121s 10ms/step - loss: 614.5013 - val_loss: 439.9410\n",
            "Epoch 41/1000\n",
            "12062/12062 [==============================] - 121s 10ms/step - loss: 614.5288 - val_loss: 447.6145\n",
            "Epoch 42/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 613.1185 - val_loss: 487.2621\n",
            "Epoch 43/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 608.7517 - val_loss: 447.6636\n",
            "Epoch 44/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 603.9185 - val_loss: 457.9894\n",
            "Epoch 45/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 604.2695 - val_loss: 444.6624\n",
            "Epoch 46/1000\n",
            "12062/12062 [==============================] - 118s 10ms/step - loss: 601.9325 - val_loss: 442.8861\n",
            "Epoch 47/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 597.0704 - val_loss: 436.4531\n",
            "Epoch 48/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 595.2286 - val_loss: 438.4723\n",
            "Epoch 49/1000\n",
            "12062/12062 [==============================] - 121s 10ms/step - loss: 598.0428 - val_loss: 437.2016\n",
            "Epoch 50/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 595.1055 - val_loss: 478.2415\n",
            "Epoch 51/1000\n",
            "12062/12062 [==============================] - 118s 10ms/step - loss: 589.5088 - val_loss: 430.5928\n",
            "Epoch 52/1000\n",
            "12062/12062 [==============================] - 121s 10ms/step - loss: 593.3150 - val_loss: 416.8502\n",
            "Epoch 53/1000\n",
            "12062/12062 [==============================] - 118s 10ms/step - loss: 589.0017 - val_loss: 420.8798\n",
            "Epoch 54/1000\n",
            "12062/12062 [==============================] - 118s 10ms/step - loss: 584.5569 - val_loss: 406.4598\n",
            "Epoch 55/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 583.3906 - val_loss: 402.7804\n",
            "Epoch 56/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 583.0214 - val_loss: 405.7207\n",
            "Epoch 57/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 579.7808 - val_loss: 394.6077\n",
            "Epoch 58/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 582.0099 - val_loss: 425.3041\n",
            "Epoch 59/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 577.2297 - val_loss: 409.2825\n",
            "Epoch 60/1000\n",
            "12062/12062 [==============================] - 123s 10ms/step - loss: 578.0137 - val_loss: 406.4670\n",
            "Epoch 61/1000\n",
            "12062/12062 [==============================] - 121s 10ms/step - loss: 578.8349 - val_loss: 387.4723\n",
            "Epoch 62/1000\n",
            "12062/12062 [==============================] - 121s 10ms/step - loss: 576.6796 - val_loss: 429.5785\n",
            "Epoch 63/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 569.6682 - val_loss: 394.1138\n",
            "Epoch 64/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 569.9915 - val_loss: 388.2027\n",
            "Epoch 65/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 571.0677 - val_loss: 418.9734\n",
            "Epoch 66/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 566.6486 - val_loss: 398.7095\n",
            "Epoch 67/1000\n",
            "12062/12062 [==============================] - 121s 10ms/step - loss: 559.2130 - val_loss: 374.5377\n",
            "Epoch 68/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 564.0397 - val_loss: 377.8465\n",
            "Epoch 69/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 559.7283 - val_loss: 382.1269\n",
            "Epoch 70/1000\n",
            "12062/12062 [==============================] - 121s 10ms/step - loss: 557.3873 - val_loss: 403.7307\n",
            "Epoch 71/1000\n",
            "12062/12062 [==============================] - 124s 10ms/step - loss: 555.2000 - val_loss: 406.1237\n",
            "Epoch 72/1000\n",
            "12062/12062 [==============================] - 126s 10ms/step - loss: 558.8806 - val_loss: 394.0777\n",
            "Epoch 73/1000\n",
            "12062/12062 [==============================] - 125s 10ms/step - loss: 558.0654 - val_loss: 368.3724\n",
            "Epoch 74/1000\n",
            "12062/12062 [==============================] - 126s 10ms/step - loss: 555.5739 - val_loss: 364.6872\n",
            "Epoch 75/1000\n",
            "12062/12062 [==============================] - 125s 10ms/step - loss: 555.3621 - val_loss: 399.2958\n",
            "Epoch 76/1000\n",
            "12062/12062 [==============================] - 126s 10ms/step - loss: 550.7777 - val_loss: 401.6969\n",
            "Epoch 77/1000\n",
            "12062/12062 [==============================] - 124s 10ms/step - loss: 552.5130 - val_loss: 362.4836\n",
            "Epoch 78/1000\n",
            "12062/12062 [==============================] - 124s 10ms/step - loss: 549.7777 - val_loss: 363.9738\n",
            "Epoch 79/1000\n",
            "12062/12062 [==============================] - 124s 10ms/step - loss: 550.4832 - val_loss: 376.0927\n",
            "Epoch 80/1000\n",
            "12062/12062 [==============================] - 124s 10ms/step - loss: 552.6589 - val_loss: 380.5215\n",
            "Epoch 81/1000\n",
            "12062/12062 [==============================] - 123s 10ms/step - loss: 543.7247 - val_loss: 379.6210\n",
            "Epoch 82/1000\n",
            "12062/12062 [==============================] - 124s 10ms/step - loss: 544.7204 - val_loss: 387.9704\n",
            "Epoch 83/1000\n",
            "12062/12062 [==============================] - 125s 10ms/step - loss: 542.8051 - val_loss: 369.3346\n",
            "Epoch 84/1000\n",
            "12062/12062 [==============================] - 127s 11ms/step - loss: 541.4492 - val_loss: 358.6461\n",
            "Epoch 85/1000\n",
            "12062/12062 [==============================] - 126s 10ms/step - loss: 536.6627 - val_loss: 354.5363\n",
            "Epoch 86/1000\n",
            "12062/12062 [==============================] - 125s 10ms/step - loss: 536.4034 - val_loss: 365.8484\n",
            "Epoch 87/1000\n",
            "12062/12062 [==============================] - 126s 10ms/step - loss: 538.8262 - val_loss: 364.8472\n",
            "Epoch 88/1000\n",
            "12062/12062 [==============================] - 125s 10ms/step - loss: 539.4180 - val_loss: 352.3771\n",
            "Epoch 89/1000\n",
            "12062/12062 [==============================] - 127s 10ms/step - loss: 538.0760 - val_loss: 357.3657\n",
            "Epoch 90/1000\n",
            "12062/12062 [==============================] - 126s 10ms/step - loss: 537.9718 - val_loss: 378.6797\n",
            "Epoch 91/1000\n",
            "12062/12062 [==============================] - 126s 10ms/step - loss: 534.2673 - val_loss: 355.8920\n",
            "Epoch 92/1000\n",
            "12062/12062 [==============================] - 126s 10ms/step - loss: 538.6492 - val_loss: 370.8464\n",
            "Epoch 93/1000\n",
            "12062/12062 [==============================] - 126s 10ms/step - loss: 536.7381 - val_loss: 348.4012\n",
            "Epoch 94/1000\n",
            "12062/12062 [==============================] - 135s 11ms/step - loss: 533.0103 - val_loss: 350.5067\n",
            "Epoch 95/1000\n",
            "12062/12062 [==============================] - 134s 11ms/step - loss: 529.2623 - val_loss: 346.4083\n",
            "Epoch 96/1000\n",
            "12062/12062 [==============================] - 133s 11ms/step - loss: 535.1852 - val_loss: 352.8366\n",
            "Epoch 97/1000\n",
            "12062/12062 [==============================] - 135s 11ms/step - loss: 527.1341 - val_loss: 363.9161\n",
            "Epoch 98/1000\n",
            "12062/12062 [==============================] - 135s 11ms/step - loss: 531.8786 - val_loss: 358.7117\n",
            "Epoch 99/1000\n",
            "12062/12062 [==============================] - 135s 11ms/step - loss: 527.5316 - val_loss: 354.9520\n",
            "Epoch 100/1000\n",
            "12062/12062 [==============================] - 129s 11ms/step - loss: 529.0042 - val_loss: 356.0317\n",
            "Epoch 101/1000\n",
            "12062/12062 [==============================] - 122s 10ms/step - loss: 527.3157 - val_loss: 358.9792\n",
            "Epoch 102/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 528.7130 - val_loss: 371.4557\n",
            "Epoch 103/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 528.1755 - val_loss: 344.7075\n",
            "Epoch 104/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 526.5281 - val_loss: 350.6217\n",
            "Epoch 105/1000\n",
            "12062/12062 [==============================] - 118s 10ms/step - loss: 521.1031 - val_loss: 340.7514\n",
            "Epoch 106/1000\n",
            "12062/12062 [==============================] - 118s 10ms/step - loss: 522.5575 - val_loss: 338.8389\n",
            "Epoch 107/1000\n",
            "12062/12062 [==============================] - 121s 10ms/step - loss: 518.4483 - val_loss: 348.3078\n",
            "Epoch 108/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 519.8312 - val_loss: 360.1226\n",
            "Epoch 109/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 523.9802 - val_loss: 341.9497\n",
            "Epoch 110/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 515.8135 - val_loss: 338.0103\n",
            "Epoch 111/1000\n",
            "12062/12062 [==============================] - 119s 10ms/step - loss: 519.6565 - val_loss: 343.8128\n",
            "Epoch 112/1000\n",
            "12062/12062 [==============================] - 117s 10ms/step - loss: 518.0807 - val_loss: 352.6066\n",
            "Epoch 113/1000\n",
            "12062/12062 [==============================] - 117s 10ms/step - loss: 519.2693 - val_loss: 360.8337\n",
            "Epoch 114/1000\n",
            "12062/12062 [==============================] - 117s 10ms/step - loss: 518.9076 - val_loss: 336.5372\n",
            "Epoch 115/1000\n",
            "12062/12062 [==============================] - 120s 10ms/step - loss: 516.4373 - val_loss: 338.1659\n",
            "Epoch 116/1000\n",
            " 3137/12062 [======>.......................] - ETA: 1:21 - loss: 528.3189"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}